{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea47f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c783070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini and grok API Key exists and begins AIzaSyC4 and gsk_MtKP\n"
     ]
    }
   ],
   "source": [
    "# Check the keys\n",
    "\n",
    "import os\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY') \n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if gemini_api_key and grok_api_key:\n",
    "    print(f\"gemini and grok API Key exists and begins {gemini_api_key[:8]} and {grok_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"gemini or grok API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0426ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Llama, a Meta-designed model built to adapt to your conversational style. Whether you need quick answers, deep dives into ideas, or just want to vent, joke or brainstorm—I'm here for it. What’s on your mind?"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=grok_api_key)\n",
    "from groq import Groq\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
